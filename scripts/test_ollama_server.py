#!/usr/bin/env python3
"""
Script pour tester la connexion au serveur Ollama et lister les mod√®les disponibles.
"""

import requests
import json
import sys
import os

# Ajouter le r√©pertoire parent au path pour importer config
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.config import OLLAMA_BASE_URL


def test_ollama_connection():
    """Test la connexion au serveur Ollama."""
    try:
        print(f"üîó Test de connexion √† {OLLAMA_BASE_URL}")
        
        # Test de l'endpoint /api/tags pour lister les mod√®les (format Ollama)
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code != 200:
            # Essayer le format OpenAI compatible
            response = requests.get(f"{OLLAMA_BASE_URL}/v1/models", timeout=10)
        
        response.raise_for_status()
        data = response.json()
        
        # Support pour les deux formats de r√©ponse
        if 'models' in data:
            models = data['models']  # Format Ollama
        elif 'data' in data:
            models = data['data']    # Format OpenAI compatible
        else:
            models = []
        
        print(f"‚úÖ Connexion r√©ussie ! {len(models)} mod√®les disponibles :")
        print("-" * 50)
        
        for model in models:
            if 'name' in model:
                # Format Ollama
                name = model.get('name', 'N/A')
                size = model.get('size', 0)
                size_gb = size / (1024**3) if size > 0 else 0
                modified = model.get('modified', 'N/A')
                
                print(f"üì¶ {name}")
                if size_gb > 0:
                    print(f"   Taille: {size_gb:.1f} GB")
                if modified != 'N/A':
                    print(f"   Modifi√©: {modified}")
            else:
                # Format OpenAI compatible
                name = model.get('id', 'N/A')
                created = model.get('created', 'N/A')
                
                print(f"üì¶ {name}")
                if created != 'N/A':
                    print(f"   Cr√©√©: {created}")
            print()
        
        return True, models
        
    except requests.exceptions.ConnectTimeout:
        print("‚ùå Timeout de connexion. V√©rifiez votre connexion r√©seau.")
        return False, []
    except requests.exceptions.ConnectionError:
        print("‚ùå Erreur de connexion. Le serveur n'est peut-√™tre pas accessible.")
        print("üí° √ätes-vous connect√© au VPN de l'universit√© ?")
        return False, []
    except requests.exceptions.HTTPError as e:
        print(f"‚ùå Erreur HTTP: {e}")
        return False, []
    except json.JSONDecodeError:
        print("‚ùå R√©ponse invalide du serveur.")
        return False, []
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {e}")
        return False, []


def test_model_availability(model_name):
    """Test si un mod√®le sp√©cifique est disponible."""
    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": model_name,
                "prompt": "Hello",
                "stream": False
            },
            timeout=30
        )
        
        if response.status_code == 200:
            print(f"‚úÖ Mod√®le '{model_name}' disponible et fonctionnel")
            return True
        else:
            print(f"‚ùå Mod√®le '{model_name}' non disponible (status: {response.status_code})")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors du test du mod√®le '{model_name}': {e}")
        return False


def main():
    """Fonction principale."""
    print("üöÄ Test du serveur Ollama GPU")
    print("=" * 50)
    
    # Test de connexion
    success, models = test_ollama_connection()
    
    if success:
        # Test des mod√®les configur√©s
        from config.config import DEFAULT_MODEL, EMBEDDING_MODEL, AI_MODELS
        
        print("üß™ Test des mod√®les configur√©s :")
        print("-" * 30)
        
        models_to_test = set([DEFAULT_MODEL, EMBEDDING_MODEL])
        for config in AI_MODELS.values():
            models_to_test.add(config["model"])
        
        for model in models_to_test:
            test_model_availability(model)
        
        print("\nüìã Mod√®les disponibles sur le serveur :")
        print("-" * 30)
        # Mod√®les bas√©s sur ce qui a √©t√© trouv√©
        available_server_models = [
            "nomic-embed-text:latest",
            "gemma3:27b",
            "gemma3:12b", 
            "gemma3:4b",
            "deepseek-r1:7b",
            "llama3:instruct",
            "mistral:latest"
        ]
        
        # Extraire les noms des mod√®les de la r√©ponse
        if models:
            if 'name' in models[0]:
                available_models = [m.get('name', '') for m in models]
            else:
                available_models = [m.get('id', '') for m in models]
        else:
            available_models = []
        
        for model in available_server_models:
            status = "‚úÖ Disponible" if any(model in available for available in available_models) else "‚ùì √Ä v√©rifier"
            print(f"  {model}: {status}")
    
    else:
        print("\nüí° Suggestions de d√©pannage :")
        print("- V√©rifiez votre connexion VPN √† l'universit√©")
        print("- Confirmez l'URL du serveur aupr√®s de votre administrateur")
        print(f"- Testez avec : curl {OLLAMA_BASE_URL}/v1/models")


if __name__ == "__main__":
    main() 